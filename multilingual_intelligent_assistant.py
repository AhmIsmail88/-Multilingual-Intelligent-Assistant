# -*- coding: utf-8 -*-
"""Multilingual Intelligent Assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gHqm2iYnD-_anAQiB3WRK1xVIlauhaoR
"""

# Installing Libraries Transformer, Torch.
# ----------------------------------------

!pip install transformers torch
!pip install sentencepiece # for Translation

# Importing Libraries
# ----------------------

from transformers import pipeline

Model = "tabularisai/multilingual-sentiment-analysis"

sentiment_analysis = pipeline(task="sentiment-analysis", model="tabularisai/multilingual-sentiment-analysis")

# Input text for sentiment analysis

text = "Water treatment is very important"

result = sentiment_analysis(text)
print ("the model resultid:", result)

txt_Gen_Model = "Qwen/Qwen2.5-3B-Instruct"

generator = pipeline(
    "text-generation",
    model=txt_Gen_Model)

topic = "water treaatment"
prompt = input ("Writr Prompt:", )

out = generator(
    prompt,
    max_new_tokens=250,
    do_sample=True,
    temperature=0.8,
    top_p=0.95
)

generated_text = out[0]["generated_text"]

print(generated_text)

# Summarization

Summarization_Model = "facebook/bart-large-cnn"
Summarization_Tokenizer = "facebook/bart-large-cnn"

# Question Answering

Question_Answering_Model = "deepset/roberta-base-squad2"
Question_Answering_Tokenizer = "deepset/roberta-base-squad2"

Summarization = pipeline(
    "summarization",
    model=Summarization_Model,
    tokenizer=Summarization_Tokenizer)

out = Summarization(
    text,
    max_length=100,
    min_length=30,
    do_sample=False)

summarized_text = out[0]["summary_text"]
print(summarized_text)

Translation_Model = "Helsinki-NLP/opus-mt-tc-big-en-fr"
translator = pipeline("translation_en_to_fr", model=Translation_Model)

out = translator(summarized_text)

print("The Translation from En to Fr is:", out)

# Q & A

Question_Answering_model = "deepset/roberta-base-squad2"
Question_Answering_tokenizer = "deepset/roberta-base-squad2"

Q_A = pipeline(
    "question-answering",
    model=Question_Answering_model,
    tokenizer=Question_Answering_tokenizer)

context = generated_text
Question = input("Enter your question: ")
Answer = Q_A(
    question=Question,
    context=context)

print("The Answer is:", Answer["answer"])